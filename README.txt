# Avalia√ß√£o de Ferramentas de Gera√ß√£o de Casos de Teste Combinat√≥rios

Este reposit√≥rio cont√©m os artefatos, scripts de automa√ß√£o, logs de execu√ß√£o e o relat√≥rio t√©cnico desenvolvido para a disciplina de **Teste de Valida√ß√£o de Software**, do Centro de Inform√°tica da Universidade Federal de Pernambuco (CIn-UFPE).

O projeto consiste em um estudo emp√≠rico comparativo entre ferramentas de **Teste Combinat√≥rio (CTG - Combinatorial Test Generation)**.

---

## üë• Equipe
* **Diego Rafael**
* **Eduardo Teles**
* **Gabriel Albertin**
* **Vin√≠cius Henrique**

üìÖ **Data:** 15 de Dezembro de 2025  
üìç **Local:** Recife, PE - Brasil

---

## üéØ Objetivo
O objetivo deste estudo √© avaliar, de forma explorat√≥ria e comparativa, o desempenho de ferramentas de gera√ß√£o de casos de teste combinat√≥rios (pairwise), considerando diferentes caracter√≠sticas dos modelos de entrada e crit√©rios de intera√ß√£o.

As ferramentas avaliadas foram:
1.  **PICT (Microsoft):** Ferramenta baseada em modelo declarativo.
2.  **Jenny:** Ferramenta baseada em especifica√ß√µes num√©ricas, conhecida por ser minimalista.

### Quest√µes de Pesquisa (QPs)
* **QP1:** Como as ferramentas diferem quanto ao tamanho dos conjuntos de teste gerados?
* **QP2:** Qual √© o tempo de execu√ß√£o requerido em diferentes cen√°rios de complexidade?
* **QP3:** Como a estabilidade das ferramentas varia entre m√∫ltiplas execu√ß√µes?
* **QP4:** Quais caracter√≠sticas influenciam mais o desempenho de cada ferramenta?

---

## üõ†Ô∏è Metodologia e Ambiente Experimental

Para garantir a validade dos dados, foi realizada uma experimenta√ß√£o controlada com **50 repeti√ß√µes** para cada cen√°rio, visando reduzir a variabilidade externa.

### Hardware Utilizado
As execu√ß√µes ocorreram em ambiente computacional fixo para garantir condi√ß√µes homog√™neas:
* **Equipamento:** Dell Inspiron 3583
* **Processador:** Intel Core i5-8265U (1.60GHz)
* **Mem√≥ria RAM:** 16 GB
* **GPU:** Intel UHD Graphics 620 (128 MB)
* **Sistema Operacional:** Windows 11

### Modelos de Teste
Foram utilizados dois modelos sint√©ticos para estressar as ferramentas:
* **M1 (Simples):** 3 par√¢metros com 3 valores cada.
* **M2 (Complexo):** 10 par√¢metros com at√© 50 valores cada (Alta complexidade combinat√≥ria).

---

## üìä Principais Resultados

A tabela abaixo resume os dados quantitativos m√©dios obtidos (tempo em milissegundos):

| Modelo | Ferramenta | Tempo M√©dio (ms) | Desvio Padr√£o (ms) | Tamanho do Conjunto (Casos) |
| :--- | :--- | :--- | :--- | :--- |
| **M1** | PICT | ~417 ms | ~112 ms | 10 |
| **M1** | Jenny | **~408 ms** | ~107 ms | **9** |
| **M2** | PICT | **~5.959 ms (5,9s)** | **~224 ms** | 3540 |
| **M2** | Jenny | ~27.941 ms (27,9s) | ~4.148 ms | **3331** |

### An√°lise Qualitativa

#### üöÄ **PICT (Microsoft)**
* **Pontos Fortes:** Extremamente r√°pido e escal√°vel em modelos complexos. Apresentou alt√≠ssima estabilidade (baixo desvio padr√£o).
* **Usabilidade:** Modelo declarativo intuitivo.
* **Recomenda√ß√£o:** Ideal para pipelines de **CI/CD** onde a velocidade e previsibilidade s√£o cr√≠ticas.

#### üìâ **Jenny**
* **Pontos Fortes:** Capacidade superior de compacta√ß√£o (gerou conjuntos ~6% menores no cen√°rio complexo).
* **Pontos Fracos:** Sofreu degrada√ß√£o severa de performance no modelo complexo (quase 5x mais lento que o PICT) e alta instabilidade entre execu√ß√µes.
* **Recomenda√ß√£o:** Ideal para cen√°rios onde o **custo de armazenamento ou execu√ß√£o manual** dos testes √© mais caro que o tempo de gera√ß√£o (foco em "tamanho do output").

---

## üìÇ Estrutura do Reposit√≥rio

* `/logs`: Cont√©m os arquivos brutos de sa√≠da das ferramentas (`*_timeX.txt`).
* `/modelos`: Arquivos de defini√ß√£o dos modelos M1 e M2 para PICT e Jenny.
* `/relatorio`: Vers√£o em PDF do relat√≥rio t√©cnico final.
* `/scripts`: Scripts utilizados para automa√ß√£o das 50 execu√ß√µes.

---

## üîó Refer√™ncias

O embasamento te√≥rico e trabalhos relacionados utilizados neste estudo incluem:
1.  *Machine Learning Applied to Software Testing: A Systematic Mapping Study* (Durelli et al.)
2.  *Research on software testing techniques and software automation testing tools* (Li et al.)
3.  *Software Testing Evolution* (Silva et al., 2025)

---
*Projeto realizado para a disciplina de Teste de Valida√ß√£o de Software - CIn/UFPE.*
